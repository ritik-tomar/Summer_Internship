# -*- coding: utf-8 -*-
"""Data_Collection_and_Preprocessing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gMwZXwfkKlyQnrdHFnxsOmrF2FInaseI
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Modules Installation**"""

!pip install -q twarc
!pip install -q jsonlines

"""## **Import Modules**"""

import os,zipfile,glob
import pandas as pd
import twarc

"""## **Total IEEE Dataset**"""

path = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_CSV_File/*.csv"
csv_list = glob.glob(path) # collecting all files  same path 
print(len(csv_list))

data = pd.DataFrame()
for f in csv_list:
  data1 = pd.read_csv(f,header=None)#reading the csv file
  data = pd.concat([data,data1],ignore_index=True)#concating the two data frames
  data.reset_index(drop=True,inplace=True) #resetting the indexes

data.rename({0:'tweetID',1:'sentiment_score'},axis=1,inplace=True)#renaming the indexes

data.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/data.csv',index=False)#converting the dataframe to CSV

for f in csv_list:
  df = pd.read_csv(f,header=None) #reading the CSV file
  df = df[0] #Only taking the Tweet Id's from the dataset
  base = os.path.basename(f) #returning the name of the file
  path = '/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweets_ID/'+base
  df.to_csv(path,index=False) # converting the dataframe to CSV

"""## **Hydrating Tweets**"""

# Insert API Keys here { run : "auto"}

# These keys are received after applying for a twitter developer account

consumer_key = "8AO6OU5ubyi4XO47b1C7Sjdlz"
consumer_secret = "FS1usPrfPolvjLXbwGka5N8TWkOZhUsdxGmmTwuO016koesUSt"
access_token = "1151573806680592384-OUFeUtpsRFZM6jQxl1AG99NEjlY0Kt"
access_token_secret = "KKHmkHkDGVaDof8XK4fKKI52DmNl4vZlaXnx85WRfd4Lr"

t = twarc.Twarc(consumer_key, consumer_secret, access_token, access_token_secret) #Initializing Twarc

import jsonlines, json

data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Geolocation_Dataset/april28-june18.csv',header=None) #Loading IEEE Geodata
data = data[0] #Taking only Tweet IDs for Hydration
ids = data.values.tolist() #Getting list of tweet ids from pandas DataFrame
hydrated_tweets = [] #Creating empty list
ids_to_hydrate = set(ids) #Creating ids_to_hydrate list

# Now, use twarc and start hydrating
for tweet in t.hydrate(ids_to_hydrate): #Initializing Hydrate Iterator with twarc
  if tweet['place'] != None: #Checking for place value
    if tweet['place']['country'] == 'India': #Checking if Country is India
      hydrated_tweets.append(tweet) #Appending Obtained tweet to hydrated_tweets list

output_filename = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Geolocation_Dataset/output.csv" #output file path

# Convert jsonl to csv
import csv, jsonlines

# These are the column name that are selected to be stored in the csv
keyset = ["created_at", "id", "id_str", "full_text", "source", "truncated", "in_reply_to_status_id",
          "in_reply_to_status_id_str", "in_reply_to_user_id", "in_reply_to_user_id_str", 
          "in_reply_to_screen_name", "user", "coordinates", "place", "quoted_status_id",
          "quoted_status_id_str", "is_quote_status", "quoted_status", "retweeted_status", 
          "quote_count", "reply_count", "retweet_count", "favorite_count", "entities", 
          "extended_entities", "favorited", "retweeted", "possibly_sensitive", "filter_level", 
          "lang", "matching_rules", "current_user_retweet", "scopes", "withheld_copyright", 
          "withheld_in_countries", "withheld_scope", "geo", "contributors", "display_text_range",
          "quoted_status_permalink"]

# Writes them out (Saving output CSV file with the Indian COVID-19 Tweets)
with  open(output_filename, "w+") as output_file:
    d = csv.DictWriter(output_file, keyset)
    d.writeheader()
    d.writerows(hydrated_tweets)

import pandas as pd

data = pd.read_csv(output_filename)# reading the file
data1 = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Geolocation_Dataset/april28-june18.csv',header=None)
data1.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns
result = pd.merge(data,data1,on='id')#merging the two dataframes
result.drop_duplicates(subset ="id", keep = False, inplace = True)#removing duplicates
result.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/India_Dataset.csv',index=False)# Converting to CSV

"""## **Sentiment Addition**"""

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/hydrated_corona_tweets_01")
df = df[['id','retweet_count','lang','text']]#selection of Features
data = df[df["lang"]=="en"] #Extracting the tweets from dataframe whose language is English

data

tweet_ids_file = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_CSV_File/corona_tweets_01.csv" #Reference Original CSV Path

data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/hydrated_corona_tweets_01') # reading the file
data.rename({'text':'full_text'},inplace=True,axis=1)
dataset = data[['id','full_text','retweet_count']]
df = pd.read_csv(tweet_ids_file,header=None)
df.rename({0:'id',1:'Sentiment'},inplace=True,axis=1)#renaming the columns
output = pd.merge(dataset,df,on='id')#merging (inner) the two dataframes
output.drop_duplicates(subset ="id", keep = False, inplace = True)#removing duplicates
output.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/Dataset.csv',index=False)# Converting to CSV

dd = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/Dataset.csv')

dd.head()

# Function to Convert the IEEE Sentiment score to Sentiment Text namely positive, negative, neutral
def func(x):
    if x < 0:
        return "negative"
    elif x == 0:
        return "neutral"
    else:
        return "positive"

df1 = pd.read_csv("/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/India_Dataset.csv")# reading the file
y= df1.Sentiment
X= df1.drop('Sentiment',axis=1)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.75,random_state=111)#Spliting the dataset 
df2 = pd.concat([X_train,y_train],axis=1)#concating the two datasets
df3 = pd.concat([X_test,y_test],axis=1)#concating the two datasets
df3.reset_index(drop=True,inplace=True)#resetting the indexes
df3['Sentiment'] = df3['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text
df3.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_India.csv',index=False)# Converting to CSV

data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Tweet_Dataset/Dataset.csv')# reading the file
df4=data.sample(n=58944,random_state=123)# sampling the dataset
df5=pd.concat([df2,df4],ignore_index=True)#Concating the two datasets 
df5.reset_index(drop=True,inplace=True)# restting the indexes
df5['Sentiment'] = df5['Sentiment'].apply(lambda x : func(x))#converting sentiment score to sentiment text
df5.to_csv("/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Data.csv",index=False)# Converting to CSV

"""## **Splitting Final Data to Train and Test sets for DL Model**"""

data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Data.csv')# reading the file
y = data.Sentiment
X = data.drop('Sentiment',axis=1)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=123)
df1 = pd.concat([X_train,y_train],axis=1)#concating the two datasets
df1.reset_index(drop=True,inplace=True)#resetting the indexes
df1.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Train_Data.csv',index=False) # Saving Training/Validation data for DL Model
df2 = pd.concat([X_test,y_test],axis=1)#concating the two datasets
df2.reset_index(drop=True,inplace=True)#resetting the indexes
df2.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_Data.csv',index=False)# Saving Test data for DL Model

"""# **Data Cleaning**"""

#Defining functions for Cleaning and Normalization of Data
import re
import string
def replace_url(string): # cleaning of URL
    text = re.sub(r'http\S+', 'LINK', string)
    return text


def replace_email(text):#Cleaning of Email related text
    line = re.sub(r'[\w\.-]+@[\w\.-]+','MAIL',str(text))
    return "".join(line)

def rep(text):#cleaning of non standard words
    grp = text.group(0)
    if len(grp) > 3:
        return grp[0:2]
    else:
        return grp# can change the value here on repetition
def unique_char(rep,sentence):
    convert = re.sub(r'(\w)\1+', rep, sentence) 
    return convert

def find_dollar(text):#Finding the dollar sign in the text
    line=re.sub(r'\$\d+(?:\.\d+)?','PRICE',text)
    return "".join(line)

def replace_emoji(text):
    emoji_pattern = re.compile("["
    u"\U0001F600-\U0001F64F" # emoticons
    u"\U0001F300-\U0001F5FF" # symbols & pictographs
    u"\U0001F680-\U0001F6FF" # transport & map symbols
    u"\U0001F1E0-\U0001F1FF" # flags (iOS)
    u"\U00002702-\U000027B0"
    u"\U000024C2-\U0001F251"
    "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'EMOJI', text)

puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', "'", '$', '&', '/', '[', ']',
          '>', '%', '=', '#', '*', '+', '\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',
          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',
          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',
          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',
          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',
          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',
          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']

def clean_text(text: str) -> str:
    text = str(text)
    for punct in puncts + list(string.punctuation):
        if punct in text:
            text = text.replace(punct, f'')
    return text
   
def replace_asterisk(text):
    text = re.sub("\*", 'ABUSE ', text)
    return text

def remove_duplicates(text):
    text = re.sub(r'\b(\w+\s*)\1{1,}', '\\1', text)
    return text

def change(text):
    if(text == ''):
        return text
  #calling the subfunctions in the cleaning function
    text = replace_email(text)
    text = replace_url(text)
    text = unique_char(rep,text)
    text = replace_asterisk(text)
    text = remove_duplicates(text)
    text = clean_text(text)
    return text

import pandas as pd

# Loading different csv files for cleaning 
pathname = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Data.csv"

Data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Data.csv') #reading the file
Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets
Data.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Data.csv') #converting to CSV

dd1 = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Data.csv')
dd1.head()

pathname = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_India.csv"

Data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_India.csv') #reading the file
Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets
Data.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Test_India.csv') #converting to CSV

dd2 = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Test_India.csv')
dd2.head()

pathname = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_Data.csv"

Data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Test_Data.csv') #reading the file
Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets
Data.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Test_Data.csv') #converting to CSV

dd3 = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Test_Data.csv')
dd3.head()

pathname = "/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Train_Data.csv"

Data = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Final_Dataset/Train_Data.csv') #reading the file
Data['full_text'] = Data['full_text'].apply(lambda x : change(x)) # Running cleaning and normalization function on datasets
Data.to_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Train_Data.csv') #converting to CSV

dd4 = pd.read_csv('/content/drive/MyDrive/Internship Dataset/Data Collection and Preprocessing/Cleaned_Dataset/Train_Data.csv')
dd4.head()